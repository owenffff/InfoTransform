# ============================================================================
# INFOTRANSFORM CACHING CONFIGURATION
# ============================================================================
#
# This file controls the result caching system for InfoTransform.
# Caching avoids re-processing identical files, significantly improving performance.
#
# HOW CACHING WORKS:
# -----------------
# 1. When a file is processed, its content is hashed (SHA-256)
# 2. The hash + model configuration = unique cache key
# 3. If an identical file is uploaded again, results are retrieved from cache
# 4. Cache entries automatically expire based on TTL (time-to-live)
#
# CACHE STORAGE:
# -------------
# - Stored in SQLite database: backend/infotransform/data/processing_logs.db
# - Only metadata and extracted JSON are cached (not original files)
# - Safe for sensitive data with configurable expiration
# - Survives app restarts (until TTL expires)
#
# PERFORMANCE IMPACT:
# ------------------
# - Cache HIT:  Results returned in <100ms (near-instant)
# - Cache MISS: Normal processing time (5-300s depending on file)
# - Typical cache hit rate: 20-40% for batch processing
#
# SECURITY CONSIDERATIONS:
# -----------------------
# - Set ttl_hours to 0 for session-only caching (clears on restart)
# - Disable caching entirely for maximum security (set enabled: false)
# - Cache keys include model configuration to prevent cross-contamination
# - No original file content is stored, only extracted structured data
#
# ============================================================================

result_cache:
  # Enable or disable result caching
  # Set to false for maximum security or if you have limited disk space
  enabled: ${CACHE_ENABLED:-true}

  # Time-to-live in hours
  # How long cached results are retained before automatic cleanup
  #
  # Common configurations:
  #   0    = Cache only during app session (clears on restart)
  #   6    = 6 hours (good for daily batch processing)
  #   24   = 24 hours (balanced - recommended default)
  #   168  = 1 week (for rarely-changing documents)
  #   720  = 1 month (for archival/reference documents)
  #
  # Note: Setting to 0 provides session-level caching without persistence
  ttl_hours: ${CACHE_TTL_HOURS:-24}

  # Maximum number of cache entries before forced cleanup
  # Prevents unbounded cache growth
  #
  # Size estimates:
  #   1,000 entries   ≈ 10-50 MB
  #   10,000 entries  ≈ 100-500 MB
  #   100,000 entries ≈ 1-5 GB
  #
  # Cleanup occurs when:
  # - Entry count exceeds this limit (oldest entries removed first)
  # - Entries exceed TTL (automatic expiration)
  # - Manual cache clear via API
  max_entries: ${CACHE_MAX_ENTRIES:-10000}

  # Cleanup interval in hours
  # How often the background cleanup task runs to remove expired entries
  # Lower values = more frequent cleanup, higher overhead
  # Higher values = less overhead, slower cache size reduction
  cleanup_interval_hours: ${CACHE_CLEANUP_INTERVAL:-6}

  # Content hashing algorithm
  # Used to generate unique cache keys from file content
  # Options: sha256 (default), sha1, md5
  #
  # Recommendation: Keep as sha256 for best collision resistance
  hash_algorithm: ${CACHE_HASH_ALGO:-sha256}

  # Cache invalidation strategy
  # Controls when cache entries are invalidated
  #
  # Options:
  #   "ttl_only"     - Only expire based on TTL (default)
  #   "model_change" - Invalidate when model version changes
  #   "aggressive"   - Invalidate on any config change
  invalidation_strategy: ${CACHE_INVALIDATION:-ttl_only}

# ============================================================================
# CACHE STATISTICS & MONITORING
# ============================================================================

monitoring:
  # Track cache performance metrics
  # Metrics include: hit rate, miss rate, average retrieval time
  enable_metrics: ${CACHE_METRICS:-true}

  # Log cache hits/misses for debugging
  # Useful for understanding cache effectiveness
  # Disable in production to reduce log volume
  log_cache_operations: ${CACHE_LOG_OPS:-false}

  # Include cache statistics in completion events
  # Shows cache hit rate in final processing summary
  include_in_completion: ${CACHE_STATS_IN_COMPLETION:-true}

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================

advanced:
  # Compress cached results to save disk space
  # Uses gzip compression on JSON data
  # Trade-off: Slightly slower retrieval, significantly less storage
  compress_results: ${CACHE_COMPRESS:-false}

  # Cache warming on startup
  # Pre-load frequently accessed results into memory cache
  # Only useful if you have predictable usage patterns
  enable_warmup: ${CACHE_WARMUP:-false}

  # Maximum size per cache entry (bytes)
  # Prevents caching of extremely large results
  # Set to 0 to disable limit
  max_entry_size_bytes: ${CACHE_MAX_ENTRY_SIZE:-1048576}  # 1MB default

# ============================================================================
# CONFIGURATION EXAMPLES
# ============================================================================
#
# DEVELOPMENT (No Caching):
# -------------------------
# result_cache:
#   enabled: false
#
# PRODUCTION (6-Hour Cache):
# --------------------------
# result_cache:
#   enabled: true
#   ttl_hours: 6
#   max_entries: 5000
#
# HIGH-SECURITY (Session Only):
# -----------------------------
# result_cache:
#   enabled: true
#   ttl_hours: 0
#   invalidation_strategy: aggressive
#
# HIGH-VOLUME (30-Day Cache):
# ---------------------------
# result_cache:
#   enabled: true
#   ttl_hours: 720
#   max_entries: 100000
#   advanced:
#     compress_results: true
#
# ============================================================================
