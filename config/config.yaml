# ============================================================================
# AI PIPELINE CONFIGURATION
# ============================================================================
# This section controls the 3-step AI transformation pipeline:
# 1. File-to-Markdown Conversion → 2. Content Summarization → 3. Structured Analysis

ai_pipeline:
  
  # Step 1: File-to-Markdown Conversion
  # Converts uploaded files (images, audio, documents) into markdown text
  markdown_conversion:
    vision_model: "openai.gpt-4.1-mini-2025-04-14"      # Model for processing images/documents
    audio_model: "whisper-1"          # Model for processing audio files
    
    # Prompt for vision processing (how to extract text from images)
    vision_prompt: |
      Please analyze this image and provide output in the following format:

      1. If the image contains text (documents, screenshots, signs, etc.):
         - Extract ALL visible text exactly as it appears
         - Preserve the original formatting, line breaks, and structure
         - Include any headers, bullet points, or special formatting
         - If there are multiple columns, process them in reading order

      2. If the image contains no text or minimal text:
         - Provide a detailed description of the image content
         - Mention key visual elements, objects, people, or scenes
         - Note any important details or context

      3. If the image contains both text and visual elements:
         - First extract all text content
         - Then provide a brief description of non-text elements

      Format your response as clean markdown.
  
  # Step 2: Content Summarization (for large files)
  # Automatically summarizes files that exceed the token threshold
  summarization:
    model: "vertex_ai.gemini-1.5-pro"  # Model used for summarization
    token_threshold: 200000             # Files over this size get summarized (set to 0 to disable)
    temperature: 0.1                    # Lower = more focused, higher = more creative
    
    # Prompt for summarization (how to condense long documents)
    prompt: |
      You are a helpful admin expert who excels at summarising long documents. 
      Your job is to produce a condensed version that still preserves all data points necessary 
      for extraction according to the following fields: {fields}.
      
      Ensure the summary retains specific details, numbers, dates, and any other critical information 
      that might be needed for data extraction. Your output should be in a report format.
  
  # Step 3: Structured Data Extraction
  # Extracts structured information from markdown content using AI analysis
  structured_analysis:
    # Default model to use
    default_model: "openai.gpt-4.1-mini-2025-04-14"

    # Models available for user selection in frontend
    available_models:
      "openai.gpt-4.1-mini-2025-04-14":
        display_name: "gpt-4.1 mini"
        temperature: 0
        seed: 123
        
      "openai.gpt-4.1-2025-04-14":
        display_name: "gpt-4.1"
        temperature: 0
        seed: 123
    
    # Streaming configuration
    streaming:
      enabled: true                    # Enable real-time streaming of results
      debounce_by: 0.01                # Delay between streaming updates (seconds)
      enable_partial: false           # Enable streaming of partial field updates (experimental)
    
    # Analysis prompts for different types of content
    prompts:
      # Default prompt used for all analysis types
      default: |
        You are an expert document analyzer. 
        Your task is to analyze markdown content and extract structured information according to the provided schema.
        Be accurate, thorough, and follow the schema exactly.
      
      # Specialized prompts for specific analysis models
      content_compliance: |
        You are a content compliance specialist. 
        Analyze documents for policy violations with high accuracy.
        Be thorough in identifying potential issues while avoiding false positives.
      
      document_metadata: |
        You are a document metadata extraction expert.
        Extract key information about documents including titles, authors, and summaries.
        Be concise but comprehensive in your analysis.
      
      technical_analysis: |
        You are a technical documentation analyst.
        Focus on identifying technical content, code snippets, and documentation quality.
        Assess complexity levels accurately based on content depth and prerequisites.
    
    # Template for analysis prompts (how custom instructions are integrated)
    prompt_template: |
      Analyze the following content.
      
      Task: {model_description}
      
      You should extract information according to the {model_name} schema.
      {custom_instructions}
      
      Content to analyze:
      
      {content}

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================
# Basic application configuration

app:
  name: "Information Transformer"
  version: "1.0.0"
  description: "Transform any file type into structured, actionable data"
  environment: "development"          # development, staging, production

# API server configuration
api:
  host: "0.0.0.0"
  port: 8000
  cors:
    allow_origins: ["*"]
    allow_credentials: true
    allow_methods: ["*"]
    allow_headers: ["*"]

# ============================================================================
# PROCESSING CONFIGURATION
# ============================================================================
# Settings that control how files are processed

processing:
  # File upload settings
  upload:
    folder: uploads
    max_file_size_mb: 16
    allowed_extensions:
      images:
        - png
        - jpg
        - jpeg
        - gif
        - bmp
        - webp
      audio:
        - mp3
        - wav
        - m4a
        - flac
        - ogg
        - webm
      documents:
        - pdf
        - docx
        - pptx
        - xlsx
        - txt
        - md
        - html
        - msg
      archives:
        - zip
  
  # Analysis performance settings
  analysis:
    max_concurrent_analyses: 5         # How many files to analyze simultaneously
    batch_size: 10                     # Files processed in each batch
    timeouts:
      single_file: 60                  # Timeout for single file analysis (seconds)
      batch: 300                       # Timeout for batch analysis (seconds)
    retry:
      max_attempts: 3                  # Retry failed analyses
      backoff_factor: 2.0              # Exponential backoff multiplier
      max_backoff: 30                  # Maximum backoff delay (seconds)
  
  # Batch processing settings
  batch:
    max_concurrent: 10                 # Maximum concurrent batch operations
    timeout_seconds: 300               # Batch operation timeout
    max_zip_size_mb: 100              # Maximum ZIP file size
    temp_extract_dir: temp_extracts    # Directory for extracting ZIP files

# ============================================================================
# STORAGE & CACHING
# ============================================================================

storage:
  # Results storage
  results:
    type: "filesystem"                 # filesystem, database, s3
    path: "./analysis_results"
    retention_days: 30
  
  # Cache configuration
  cache:
    enabled: true
    type: "memory"                     # memory, redis
    ttl: 3600                         # Cache lifetime in seconds
    max_size: "1000MB"

# ============================================================================
# FEATURE FLAGS
# ============================================================================
# Enable/disable specific features

features:
  streaming_responses: true            # Stream results in real-time
  batch_processing: true               # Allow processing multiple files
  result_caching: true                 # Cache analysis results
  auto_retry: true                     # Automatically retry failed operations
  detailed_errors: true                # Show detailed error messages (development only)
  
  # Experimental features (use with caution)
  experimental:
    parallel_analysis: false           # Parallel processing of analyses
    smart_batching: false              # Intelligent batch size adjustment
    adaptive_prompts: false            # Dynamic prompt optimization

# ============================================================================
# SECURITY
# ============================================================================

security:
  # API security
  api_key_required: false              # Set to true in production
  api_key_header: "X-API-Key"
  
  # Input validation
  input_validation:
    max_custom_instructions_length: 1000
    sanitize_filenames: true
